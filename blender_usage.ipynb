{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Blender Usage examples\n",
    "\n",
    "## Loading blender (quick start)\n",
    "You can find more custom configurations in \n",
    "- PairRanker: [./llm_blender/pair_ranker/config.py](./llm_blender/pair_ranker/config.py)\n",
    "- GenFuser: [./llm_blender/gen_fuser/config.py](./llm_blender/gen_fuser/config.py)\n",
    "- Blender: [./llm_blender/blender/config.py](./llm_blender/blender/config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "WARNING:root:Checkpoint 'OpenAssistant/reward-model-deberta-v3-large-v2' does not exist\n",
      "WARNING:root:Try dowloading checkpoint from huggingface hub: OpenAssistant/reward-model-deberta-v3-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ed5b11faef4f22acc12d55a7c77e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77212487b411421ab58aa51cb3c90c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f518bed853b0417bbe9403fa33bc2629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01528b1c29f4b5b832b590b42e2e9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739867fd8c2d4340909068f7db1c1209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d23a7baae64b31b31e1268282cbde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9a6a1d41c048f896db506455f8bb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/993 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4088aabaeb8445a298ed8a09e2835471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d56911c838b49e996ed82f2bb67a637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84841097b5ab4e368545cae27d4e8679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62af86d150464efda0ccfefa5b6414c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Successfully downloaded checkpoint to '/Users/fangyuanyu/.cache/huggingface/hub/OpenAssistant/reward-model-deberta-v3-large-v2'\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/dataclasses_json/core.py:188: RuntimeWarning: 'NoneType' object value of non-optional type cache_dir detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/dataclasses_json/core.py:188: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:root:Missing keys: ['pretrained_model.embeddings.word_embeddings.weight', 'pretrained_model.embeddings.LayerNorm.weight', 'pretrained_model.embeddings.LayerNorm.bias', 'pretrained_model.encoder.layer.0.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.0.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.0.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.0.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.0.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.0.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.0.attention.output.dense.weight', 'pretrained_model.encoder.layer.0.attention.output.dense.bias', 'pretrained_model.encoder.layer.0.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.0.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.0.intermediate.dense.weight', 'pretrained_model.encoder.layer.0.intermediate.dense.bias', 'pretrained_model.encoder.layer.0.output.dense.weight', 'pretrained_model.encoder.layer.0.output.dense.bias', 'pretrained_model.encoder.layer.0.output.LayerNorm.weight', 'pretrained_model.encoder.layer.0.output.LayerNorm.bias', 'pretrained_model.encoder.layer.1.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.1.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.1.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.1.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.1.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.1.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.1.attention.output.dense.weight', 'pretrained_model.encoder.layer.1.attention.output.dense.bias', 'pretrained_model.encoder.layer.1.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.1.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.1.intermediate.dense.weight', 'pretrained_model.encoder.layer.1.intermediate.dense.bias', 'pretrained_model.encoder.layer.1.output.dense.weight', 'pretrained_model.encoder.layer.1.output.dense.bias', 'pretrained_model.encoder.layer.1.output.LayerNorm.weight', 'pretrained_model.encoder.layer.1.output.LayerNorm.bias', 'pretrained_model.encoder.layer.2.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.2.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.2.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.2.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.2.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.2.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.2.attention.output.dense.weight', 'pretrained_model.encoder.layer.2.attention.output.dense.bias', 'pretrained_model.encoder.layer.2.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.2.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.2.intermediate.dense.weight', 'pretrained_model.encoder.layer.2.intermediate.dense.bias', 'pretrained_model.encoder.layer.2.output.dense.weight', 'pretrained_model.encoder.layer.2.output.dense.bias', 'pretrained_model.encoder.layer.2.output.LayerNorm.weight', 'pretrained_model.encoder.layer.2.output.LayerNorm.bias', 'pretrained_model.encoder.layer.3.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.3.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.3.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.3.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.3.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.3.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.3.attention.output.dense.weight', 'pretrained_model.encoder.layer.3.attention.output.dense.bias', 'pretrained_model.encoder.layer.3.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.3.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.3.intermediate.dense.weight', 'pretrained_model.encoder.layer.3.intermediate.dense.bias', 'pretrained_model.encoder.layer.3.output.dense.weight', 'pretrained_model.encoder.layer.3.output.dense.bias', 'pretrained_model.encoder.layer.3.output.LayerNorm.weight', 'pretrained_model.encoder.layer.3.output.LayerNorm.bias', 'pretrained_model.encoder.layer.4.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.4.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.4.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.4.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.4.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.4.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.4.attention.output.dense.weight', 'pretrained_model.encoder.layer.4.attention.output.dense.bias', 'pretrained_model.encoder.layer.4.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.4.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.4.intermediate.dense.weight', 'pretrained_model.encoder.layer.4.intermediate.dense.bias', 'pretrained_model.encoder.layer.4.output.dense.weight', 'pretrained_model.encoder.layer.4.output.dense.bias', 'pretrained_model.encoder.layer.4.output.LayerNorm.weight', 'pretrained_model.encoder.layer.4.output.LayerNorm.bias', 'pretrained_model.encoder.layer.5.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.5.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.5.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.5.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.5.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.5.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.5.attention.output.dense.weight', 'pretrained_model.encoder.layer.5.attention.output.dense.bias', 'pretrained_model.encoder.layer.5.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.5.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.5.intermediate.dense.weight', 'pretrained_model.encoder.layer.5.intermediate.dense.bias', 'pretrained_model.encoder.layer.5.output.dense.weight', 'pretrained_model.encoder.layer.5.output.dense.bias', 'pretrained_model.encoder.layer.5.output.LayerNorm.weight', 'pretrained_model.encoder.layer.5.output.LayerNorm.bias', 'pretrained_model.encoder.layer.6.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.6.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.6.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.6.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.6.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.6.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.6.attention.output.dense.weight', 'pretrained_model.encoder.layer.6.attention.output.dense.bias', 'pretrained_model.encoder.layer.6.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.6.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.6.intermediate.dense.weight', 'pretrained_model.encoder.layer.6.intermediate.dense.bias', 'pretrained_model.encoder.layer.6.output.dense.weight', 'pretrained_model.encoder.layer.6.output.dense.bias', 'pretrained_model.encoder.layer.6.output.LayerNorm.weight', 'pretrained_model.encoder.layer.6.output.LayerNorm.bias', 'pretrained_model.encoder.layer.7.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.7.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.7.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.7.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.7.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.7.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.7.attention.output.dense.weight', 'pretrained_model.encoder.layer.7.attention.output.dense.bias', 'pretrained_model.encoder.layer.7.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.7.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.7.intermediate.dense.weight', 'pretrained_model.encoder.layer.7.intermediate.dense.bias', 'pretrained_model.encoder.layer.7.output.dense.weight', 'pretrained_model.encoder.layer.7.output.dense.bias', 'pretrained_model.encoder.layer.7.output.LayerNorm.weight', 'pretrained_model.encoder.layer.7.output.LayerNorm.bias', 'pretrained_model.encoder.layer.8.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.8.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.8.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.8.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.8.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.8.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.8.attention.output.dense.weight', 'pretrained_model.encoder.layer.8.attention.output.dense.bias', 'pretrained_model.encoder.layer.8.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.8.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.8.intermediate.dense.weight', 'pretrained_model.encoder.layer.8.intermediate.dense.bias', 'pretrained_model.encoder.layer.8.output.dense.weight', 'pretrained_model.encoder.layer.8.output.dense.bias', 'pretrained_model.encoder.layer.8.output.LayerNorm.weight', 'pretrained_model.encoder.layer.8.output.LayerNorm.bias', 'pretrained_model.encoder.layer.9.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.9.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.9.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.9.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.9.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.9.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.9.attention.output.dense.weight', 'pretrained_model.encoder.layer.9.attention.output.dense.bias', 'pretrained_model.encoder.layer.9.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.9.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.9.intermediate.dense.weight', 'pretrained_model.encoder.layer.9.intermediate.dense.bias', 'pretrained_model.encoder.layer.9.output.dense.weight', 'pretrained_model.encoder.layer.9.output.dense.bias', 'pretrained_model.encoder.layer.9.output.LayerNorm.weight', 'pretrained_model.encoder.layer.9.output.LayerNorm.bias', 'pretrained_model.encoder.layer.10.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.10.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.10.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.10.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.10.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.10.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.10.attention.output.dense.weight', 'pretrained_model.encoder.layer.10.attention.output.dense.bias', 'pretrained_model.encoder.layer.10.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.10.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.10.intermediate.dense.weight', 'pretrained_model.encoder.layer.10.intermediate.dense.bias', 'pretrained_model.encoder.layer.10.output.dense.weight', 'pretrained_model.encoder.layer.10.output.dense.bias', 'pretrained_model.encoder.layer.10.output.LayerNorm.weight', 'pretrained_model.encoder.layer.10.output.LayerNorm.bias', 'pretrained_model.encoder.layer.11.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.11.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.11.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.11.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.11.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.11.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.11.attention.output.dense.weight', 'pretrained_model.encoder.layer.11.attention.output.dense.bias', 'pretrained_model.encoder.layer.11.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.11.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.11.intermediate.dense.weight', 'pretrained_model.encoder.layer.11.intermediate.dense.bias', 'pretrained_model.encoder.layer.11.output.dense.weight', 'pretrained_model.encoder.layer.11.output.dense.bias', 'pretrained_model.encoder.layer.11.output.LayerNorm.weight', 'pretrained_model.encoder.layer.11.output.LayerNorm.bias', 'pretrained_model.encoder.layer.12.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.12.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.12.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.12.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.12.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.12.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.12.attention.output.dense.weight', 'pretrained_model.encoder.layer.12.attention.output.dense.bias', 'pretrained_model.encoder.layer.12.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.12.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.12.intermediate.dense.weight', 'pretrained_model.encoder.layer.12.intermediate.dense.bias', 'pretrained_model.encoder.layer.12.output.dense.weight', 'pretrained_model.encoder.layer.12.output.dense.bias', 'pretrained_model.encoder.layer.12.output.LayerNorm.weight', 'pretrained_model.encoder.layer.12.output.LayerNorm.bias', 'pretrained_model.encoder.layer.13.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.13.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.13.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.13.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.13.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.13.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.13.attention.output.dense.weight', 'pretrained_model.encoder.layer.13.attention.output.dense.bias', 'pretrained_model.encoder.layer.13.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.13.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.13.intermediate.dense.weight', 'pretrained_model.encoder.layer.13.intermediate.dense.bias', 'pretrained_model.encoder.layer.13.output.dense.weight', 'pretrained_model.encoder.layer.13.output.dense.bias', 'pretrained_model.encoder.layer.13.output.LayerNorm.weight', 'pretrained_model.encoder.layer.13.output.LayerNorm.bias', 'pretrained_model.encoder.layer.14.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.14.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.14.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.14.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.14.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.14.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.14.attention.output.dense.weight', 'pretrained_model.encoder.layer.14.attention.output.dense.bias', 'pretrained_model.encoder.layer.14.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.14.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.14.intermediate.dense.weight', 'pretrained_model.encoder.layer.14.intermediate.dense.bias', 'pretrained_model.encoder.layer.14.output.dense.weight', 'pretrained_model.encoder.layer.14.output.dense.bias', 'pretrained_model.encoder.layer.14.output.LayerNorm.weight', 'pretrained_model.encoder.layer.14.output.LayerNorm.bias', 'pretrained_model.encoder.layer.15.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.15.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.15.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.15.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.15.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.15.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.15.attention.output.dense.weight', 'pretrained_model.encoder.layer.15.attention.output.dense.bias', 'pretrained_model.encoder.layer.15.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.15.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.15.intermediate.dense.weight', 'pretrained_model.encoder.layer.15.intermediate.dense.bias', 'pretrained_model.encoder.layer.15.output.dense.weight', 'pretrained_model.encoder.layer.15.output.dense.bias', 'pretrained_model.encoder.layer.15.output.LayerNorm.weight', 'pretrained_model.encoder.layer.15.output.LayerNorm.bias', 'pretrained_model.encoder.layer.16.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.16.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.16.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.16.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.16.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.16.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.16.attention.output.dense.weight', 'pretrained_model.encoder.layer.16.attention.output.dense.bias', 'pretrained_model.encoder.layer.16.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.16.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.16.intermediate.dense.weight', 'pretrained_model.encoder.layer.16.intermediate.dense.bias', 'pretrained_model.encoder.layer.16.output.dense.weight', 'pretrained_model.encoder.layer.16.output.dense.bias', 'pretrained_model.encoder.layer.16.output.LayerNorm.weight', 'pretrained_model.encoder.layer.16.output.LayerNorm.bias', 'pretrained_model.encoder.layer.17.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.17.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.17.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.17.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.17.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.17.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.17.attention.output.dense.weight', 'pretrained_model.encoder.layer.17.attention.output.dense.bias', 'pretrained_model.encoder.layer.17.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.17.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.17.intermediate.dense.weight', 'pretrained_model.encoder.layer.17.intermediate.dense.bias', 'pretrained_model.encoder.layer.17.output.dense.weight', 'pretrained_model.encoder.layer.17.output.dense.bias', 'pretrained_model.encoder.layer.17.output.LayerNorm.weight', 'pretrained_model.encoder.layer.17.output.LayerNorm.bias', 'pretrained_model.encoder.layer.18.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.18.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.18.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.18.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.18.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.18.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.18.attention.output.dense.weight', 'pretrained_model.encoder.layer.18.attention.output.dense.bias', 'pretrained_model.encoder.layer.18.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.18.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.18.intermediate.dense.weight', 'pretrained_model.encoder.layer.18.intermediate.dense.bias', 'pretrained_model.encoder.layer.18.output.dense.weight', 'pretrained_model.encoder.layer.18.output.dense.bias', 'pretrained_model.encoder.layer.18.output.LayerNorm.weight', 'pretrained_model.encoder.layer.18.output.LayerNorm.bias', 'pretrained_model.encoder.layer.19.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.19.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.19.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.19.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.19.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.19.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.19.attention.output.dense.weight', 'pretrained_model.encoder.layer.19.attention.output.dense.bias', 'pretrained_model.encoder.layer.19.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.19.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.19.intermediate.dense.weight', 'pretrained_model.encoder.layer.19.intermediate.dense.bias', 'pretrained_model.encoder.layer.19.output.dense.weight', 'pretrained_model.encoder.layer.19.output.dense.bias', 'pretrained_model.encoder.layer.19.output.LayerNorm.weight', 'pretrained_model.encoder.layer.19.output.LayerNorm.bias', 'pretrained_model.encoder.layer.20.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.20.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.20.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.20.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.20.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.20.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.20.attention.output.dense.weight', 'pretrained_model.encoder.layer.20.attention.output.dense.bias', 'pretrained_model.encoder.layer.20.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.20.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.20.intermediate.dense.weight', 'pretrained_model.encoder.layer.20.intermediate.dense.bias', 'pretrained_model.encoder.layer.20.output.dense.weight', 'pretrained_model.encoder.layer.20.output.dense.bias', 'pretrained_model.encoder.layer.20.output.LayerNorm.weight', 'pretrained_model.encoder.layer.20.output.LayerNorm.bias', 'pretrained_model.encoder.layer.21.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.21.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.21.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.21.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.21.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.21.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.21.attention.output.dense.weight', 'pretrained_model.encoder.layer.21.attention.output.dense.bias', 'pretrained_model.encoder.layer.21.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.21.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.21.intermediate.dense.weight', 'pretrained_model.encoder.layer.21.intermediate.dense.bias', 'pretrained_model.encoder.layer.21.output.dense.weight', 'pretrained_model.encoder.layer.21.output.dense.bias', 'pretrained_model.encoder.layer.21.output.LayerNorm.weight', 'pretrained_model.encoder.layer.21.output.LayerNorm.bias', 'pretrained_model.encoder.layer.22.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.22.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.22.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.22.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.22.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.22.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.22.attention.output.dense.weight', 'pretrained_model.encoder.layer.22.attention.output.dense.bias', 'pretrained_model.encoder.layer.22.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.22.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.22.intermediate.dense.weight', 'pretrained_model.encoder.layer.22.intermediate.dense.bias', 'pretrained_model.encoder.layer.22.output.dense.weight', 'pretrained_model.encoder.layer.22.output.dense.bias', 'pretrained_model.encoder.layer.22.output.LayerNorm.weight', 'pretrained_model.encoder.layer.22.output.LayerNorm.bias', 'pretrained_model.encoder.layer.23.attention.self.query_proj.weight', 'pretrained_model.encoder.layer.23.attention.self.query_proj.bias', 'pretrained_model.encoder.layer.23.attention.self.key_proj.weight', 'pretrained_model.encoder.layer.23.attention.self.key_proj.bias', 'pretrained_model.encoder.layer.23.attention.self.value_proj.weight', 'pretrained_model.encoder.layer.23.attention.self.value_proj.bias', 'pretrained_model.encoder.layer.23.attention.output.dense.weight', 'pretrained_model.encoder.layer.23.attention.output.dense.bias', 'pretrained_model.encoder.layer.23.attention.output.LayerNorm.weight', 'pretrained_model.encoder.layer.23.attention.output.LayerNorm.bias', 'pretrained_model.encoder.layer.23.intermediate.dense.weight', 'pretrained_model.encoder.layer.23.intermediate.dense.bias', 'pretrained_model.encoder.layer.23.output.dense.weight', 'pretrained_model.encoder.layer.23.output.dense.bias', 'pretrained_model.encoder.layer.23.output.LayerNorm.weight', 'pretrained_model.encoder.layer.23.output.LayerNorm.bias', 'pretrained_model.encoder.rel_embeddings.weight', 'pretrained_model.encoder.LayerNorm.weight', 'pretrained_model.encoder.LayerNorm.bias', 'head_layer.1.weight', 'head_layer.1.bias', 'head_layer.4.weight', 'head_layer.4.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /Users/fangyuanyu/.cache/huggingface/hub/OpenAssistant/reward-model-deberta-v3-large-v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import llm_blender_mps as llm_blender\n",
    "\n",
    "blender = llm_blender.Blender()\n",
    "# Load Ranker\n",
    "# blender.loadranker(\"llm-blender/PairRM\") # load ranker checkpoint\n",
    "blender.loadranker(\"OpenAssistant/reward-model-deberta-v3-large-v2\") # load ranker checkpoint\n",
    "# Load Fuser\n",
    "# blender.loadfuser(\"llm-blender/gen_fuser_3b\") # load fuser checkpoint if you want to use pre-trained fuser; or you can use ranker only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /Users/fangyuanyu/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "# Hacking into MPS compatible mode\n",
    "from llm_blender.blender.config import BlenderConfig\n",
    "device = torch.device(\"mps\")\n",
    "blender_config = BlenderConfig(device=torch.device(\"mps\"))\n",
    "blender = llm_blender.Blender(blender_config=blender_config)\n",
    "blender.loadranker(\"llm-blender/PairRM\", device=device) # load ranker checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 2]\n",
      " [1 3 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"hello, how are you!\", \"I love you!\"]\n",
    "candidates_texts = [[\"get out!\", \"hi! I am fine, thanks!\", \"bye!\"], \n",
    "                    [\"I love you too!\", \"I hate you!\", \"Thanks! You're a good guy!\"]]\n",
    "ranks = blender.rank(inputs, candidates_texts, return_scores=False, batch_size=1)\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mixinstruct dataset for the following examples showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid pattern: '**' can only be an entire path component",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm_blender_mps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcor_eval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COR_MAPS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm_blender_mps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ranks_from_chatgpt_cmps\n\u001b[0;32m----> 5\u001b[0m mixinstruct_test \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm-blender/mix-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m few_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(mixinstruct_test\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# remove cmp_results with none cmp results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1769\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1770\u001b[0m )\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1774\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1775\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1776\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1777\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1778\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1779\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   1780\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1781\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1782\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1783\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   1784\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   1786\u001b[0m )\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1502\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1503\u001b[0m     path,\n\u001b[1;32m   1504\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1505\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1506\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1507\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1508\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1219\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1215\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1218\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1203\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1189\u001b[0m             path,\n\u001b[1;32m   1190\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m             dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1194\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1197\u001b[0m             path,\n\u001b[1;32m   1198\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1199\u001b[0m             data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1200\u001b[0m             data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1201\u001b[0m             download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1202\u001b[0m             download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m-> 1203\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:769\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetModule:\n\u001b[1;32m    760\u001b[0m     hfh_dataset_info \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    762\u001b[0m         revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    763\u001b[0m         token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token,\n\u001b[1;32m    764\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    767\u001b[0m         sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns_in_dataset_repository(hfh_dataset_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    770\u001b[0m     )\n\u001b[1;32m    771\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    772\u001b[0m         patterns,\n\u001b[1;32m    773\u001b[0m         dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    774\u001b[0m         base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[1;32m    775\u001b[0m         allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    776\u001b[0m     )\n\u001b[1;32m    777\u001b[0m     split_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    778\u001b[0m         split: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m split, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    780\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/data_files.py:662\u001b[0m, in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[0;34m(dataset_info, base_path)\u001b[0m\n\u001b[1;32m    660\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info, base_path\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_data_files_patterns(resolver)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyDatasetError(\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/data_files.py:223\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m--> 223\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m pattern_resolver(pattern)\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    225\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/data_files.py:473\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[0;34m(dataset_info, pattern, base_path, allowed_extensions)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 473\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mglob(PurePath(pattern)\u001b[38;5;241m.\u001b[39mas_posix()) \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[1;32m    474\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    475\u001b[0m     filepath\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fsspec/spec.py:606\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    604\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 606\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    607\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[1;32m    609\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    610\u001b[0m     p: info\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fsspec/utils.py:734\u001b[0m, in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[1;32m    738\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "from llm_blender_mps.gpt_eval.cor_eval import COR_MAPS\n",
    "from llm_blender_mps.gpt_eval.utils import get_ranks_from_chatgpt_cmps\n",
    "mixinstruct_test = datasets.load_dataset(\"llm-blender/mix-instruct\", split=\"test\", streaming=True)\n",
    "few_examples = list(mixinstruct_test.take(8))\n",
    "# remove cmp_results with none cmp results\n",
    "for ex in few_examples:\n",
    "    ex['cmp_results'] = json.loads(ex['cmp_results'])\n",
    "few_examples = [x for x in few_examples if x['cmp_results']]\n",
    "insts = [x['instruction'] for x in few_examples]\n",
    "inputs = [x['input'] for x in few_examples]\n",
    "candidates_texts = [[cand['text'] for cand in x['candidates']] for x in few_examples]\n",
    "print(\"Example:\")\n",
    "print(\"Instruction 1:\\n\", insts[0])\n",
    "print(\"Input 1:\\n\", inputs[0])\n",
    "print(\"Candidate 1 for input 1:\\n\")\n",
    "print(candidates_texts[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1: Using LLM-Blender for ranking\n",
    "By the rank function, LLM-Blender could ranks the candidates through pairwise comparisons and return the ranks. We show our ranker's ranks are highly correlated with ChatGPT ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = blender.rank(inputs, candidates_texts, instructions=insts, return_scores=False, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ranks for input 1:\", ranks[0]) # ranks of candidates for input 1\n",
    "# Ranks for input 1: [ 1 11  4  9 12  5  2  8  6  3 10  7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "llm_ranks_map, gpt_cmp_results = get_ranks_from_chatgpt_cmps(few_examples)\n",
    "gpt_ranks = np.array(list(llm_ranks_map.values())).T\n",
    "print(\"Correlation with ChatGPT\")\n",
    "print(\"------------------------\")\n",
    "for cor_name, cor_func in COR_MAPS.items():\n",
    "    print(cor_name, cor_func(ranks, gpt_ranks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 2: Using LLM-blender to directly compare two candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_A = [x['candidates'][0]['text'] for x in few_examples]\n",
    "candidates_B = [x['candidates'][1]['text'] for x in few_examples]\n",
    "comparison_results = blender.compare(\n",
    "    inputs, candidates_A, candidates_B, instructions=insts, \n",
    "    batch_size=2, return_logits=False)\n",
    "print(\"Comparison results for inputs:\", comparison_results) # comparison results for input 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 3: Using LLM-Blender for fuse generation\n",
    "We show that the the fused generation using the top-ranked candidate from the rankers could get outputs of higher quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_blender.blender.blender_utils import get_topk_candidates_from_ranks\n",
    "topk_candidates = get_topk_candidates_from_ranks(ranks, candidates_texts, top_k=3)\n",
    "fuse_generations = blender.fuse(inputs, topk_candidates, instructions=insts, batch_size=2)\n",
    "print(\"fuse_generations for input 1:\", fuse_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Or do rank and fuser together\n",
    "fuse_generations, ranks = blender.rank_and_fuse(inputs, candidates_texts, instructions=insts, return_scores=False, batch_size=2, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_blender.common.evaluation import overall_eval\n",
    "metrics = ['bartscore']\n",
    "targets = [x['output'] for x in few_examples]\n",
    "scores = overall_eval(fuse_generations, targets, metrics)\n",
    "\n",
    "print(\"Fusion Scores\")\n",
    "for key, value in scores.items():\n",
    "    print(\"  \", key+\":\", np.mean(value))\n",
    "\n",
    "print(\"LLM Scores\")\n",
    "llms = [x['model'] for x in few_examples[0]['candidates']]\n",
    "llm_scores_map = {llm: {metric: [] for metric in metrics} for llm in llms}\n",
    "for ex in few_examples:\n",
    "    for cand in ex['candidates']:\n",
    "        for metric in metrics:\n",
    "            llm_scores_map[cand['model']][metric].append(cand['scores'][metric])\n",
    "for i, (llm, scores_map) in enumerate(llm_scores_map.items()):\n",
    "    print(f\"{i} {llm}\")\n",
    "    for metric, llm_scores in llm_scores_map[llm].items():\n",
    "        print(\"  \", metric+\":\", \"{:.4f}\".format(np.mean(llm_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 4: Use LLM-Blender for decoding enhancement (best-of-n sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\")\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "}\n",
    "messages = [\n",
    "    [   \n",
    "        system_message,\n",
    "        {\"role\": \"user\", \"content\": _inst + \"\\n\" + _input},\n",
    "    ]\n",
    "    for _inst, _input in zip(insts, inputs)\n",
    "]\n",
    "prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]\n",
    "outputs = blender.best_of_n_generate(model, tokenizer, prompts, n=10)\n",
    "print(\"### Prompt:\")\n",
    "print(prompts[0])\n",
    "print(\"### best-of-n generations:\")\n",
    "print(outputs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 5: Use PairRM for RLHF tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_reranker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
